# Data

-   Sources
-   Description
-   Wrangling/cleaning
-   Spotting mistakes and missing data (could be part of EDA too)
-   Listing anomalies and outliers (could be part of EDA too)


### Swiss Production
### Sources

```{python}
# import data
data = pd.read_csv('../data/Données_marché_Lait.csv')
print('Dataset length:', len(data))
data.head()

# Check the number of unique values in each column
data.nunique()

```

Source - https://www.donnees-agrimarche.ch/donnees

### Description

This data set shows the price of organic and conventional milk production from Switzerland and other countries over time.

### Wrangling and Cleaning 

We start by removing the columns with no value or unique value given that they are not relevant for our analysis. 
After proceeding, we end up with a sub-data set of 9 variables.
Then, we calculate the number of missing values for each characteristics and filter out those without.

```{python}
clean_data = data.copy()

# Drop columns with unique values 
clean_data = clean_data.drop(['Devise', 'Groupe de produits', 'Commerce extérieur', 'Indicateur',\
                   'Marché', 'Propriétés du produit', 'Unité', 'Echelon de création de valeur '], axis=1)

# Calculate the number of NaN for each column and filter out those without NaN
nan_counts = clean_data.isna().sum()
nan_counts = nan_counts[nan_counts > 0]
print(nan_counts)

# Create a barplot
plt.figure(figsize=(10, 6))
sns.barplot(x=nan_counts.index, y=nan_counts.values)
plt.title('Nombre de valeurs manquantes par colonne')
plt.xlabel('Colonnes')
plt.ylabel('Nombre de NaN')

plt.show()

# Calculate the percentage of NaN for each column of this cleaned data set 
nan_percent = clean_data.isna().mean() * 100

# Filter to keep only columns with NaN
nan_percent = nan_percent[nan_percent > 0].reset_index()
nan_percent.columns = ['Colonne', 'Pourcentage de NaN']

# Create an interactive graph with Plotly
fig = px.bar(nan_percent, x='Colonne', y='Pourcentage de NaN',
             text='Pourcentage de NaN')

fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')
fig.update_yaxes(range=[0, 100])
fig.update_layout(title_text='Pourcentage de valeurs manquantes par colonne',
                  yaxis_title='Pourcentage de NaN',
                  xaxis_title='Colonne')
fig.show()
```

The columns with missing values are: Cost components, Production system, Sales region and Use mode.
The 'Production system' variable has a higher percentage of missing values, around 37.6%. However, this is a key variable in our analysis so we keep it and replace missing information 'NaN' with 'Unknown' for clarity. 
As for the other characteristics, their percentage of missing values is relatively low thus we keep them too.

### General Analysis - All Countries
## Milk Production by country 

The milk sold in Switzerland comes from 14 different places - 14 values in 'Origin of product'. It would be interesting to see the distribution of the origin of milk in Switzerland, with the percentage of milk coming from Switzerland and the percentage of milk not coming from Switzerland.
We create a new variable 'swiss_production_data' including all five Swiss regions. Then, we group all non-Swiss origins together in another category. This gives us:

```{python}

# create a new variable including all Swiss regions
swiss_production_data = ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5'] 

# Create a new column for classification
clean_data['Provenance'] = clean_data['Provenance du produit'].apply(lambda x: 'Suisse' if x in swiss_production_data else 'Etranger')

# Calculate percentage for both categories
category_counts = clean_data['Provenance'].value_counts(normalize=True) * 100

# Plot the distribution
plt.figure(figsize=(8, 4))
plt.bar(category_counts.index, category_counts.values, color=['blue', 'red'])
plt.xlabel('Origin')
plt.ylabel('Percentage (%)')
plt.title('Distribution between Swiss and foreign milk')
plt.ylim(0, 100)  
plt.show()

print(category_counts)

```

The histogram shows a higher percentage of milk comes from Switzerland than from imports. However the difference is small, less than 10%.
This suggests that there are not many Swiss producers in comparison to the population.

## Milk Price by country 

Now that we've compared the distribution of production, we'll compare the distribution of the price of milk by country.

```{python}

# Convert date from string to datetime
clean_data['Date'] = pd.to_datetime(clean_data['Date'])

# Replace the country names with 'Group'
clean_data['Provenance du produit'] = clean_data['Provenance du produit'].replace(swiss_production_data, 'Suisse')

# Group the data by 'Origin of product' & 'Date' and calculate the average price. 
prix_moyen_par_pays_et_annee = clean_data.groupby(['Provenance du produit', 'Date'])['Prix'].mean().reset_index()

# Plot
plt.figure(figsize=(12, 6))
# Loop over each country to trace it
for pays in prix_moyen_par_pays_et_annee['Provenance du produit'].unique():
    subset = prix_moyen_par_pays_et_annee[prix_moyen_par_pays_et_annee['Provenance du produit'] == pays]
    plt.plot(subset['Date'], subset['Prix'], label=pays)
    
plt.legend()
plt.title('Average milk production price by country and year')
plt.xlabel('Year')
plt.ylabel('Average milk production price')
plt.legend(title='Origin', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)

plt.gca().xaxis.set_major_locator(mdates.YearLocator()) 
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y')) 

plt.tight_layout()  
plt.show()

```

We observe from this plot that producing milk in Switzerland is much more expensive than in other countries.

### Univariate Analysis - Switzerland

The main objective of this project is to focus on milk producers in Switzerland and more specifically on their remuneration. 
To do this, we are going to use the variable 'swiss_production_data' which includes all the Swiss production and sales regions.

## Distribution of milk production for each Swiss region.

# créer un tableau affichant les régions

```{python}
# Count the occurrences of each type of product
product_counts = swiss_production_data['Provenance du produit'].value_counts()

# Define the desired order of regions
order = ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5']

# Re-index the data in the desired order
product_counts = product_counts.reindex(order)

# Create a bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(product_counts.index, product_counts.values, color=['red', 'green', 'blue', 'orange', 'purple'])

# Detailed descriptions for each region 
labels = ['Région 1 : Genève, Vaud, Fribourg, Neuchâtel, Jura', 
          'Région 2 : Berne, Lucerne, Unterwald, Uri, Zoug and Schwyz', 
          'Région 3 : Bâle-Campagne & Bâle-Ville, Argovie and Soleure.',
          'Région 4 : Zurich, Schaffhouse, Thurgovie, Appenzell, St.-Gall, Schwyz, Glaris and Grisons',
          'Région 5 : Valais and Tessin.']

for bar, label in zip(bars, labels):
    bar.set_label(label)
    
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))


plt.title('Distribution de la production de lait en Suisse')
plt.xlabel('Provenance du lait')
plt.ylabel('Nombre d’Occurrences')

plt.show()

print(product_counts)
```

We can see that regions 1, 2 and 4 produce a greater quantity of milk than regions 3 and 5. 

## Cost of production 

Given that the sample size differs for each region, we represent the distribution of the cost of production for each region in terms of density to be able to compare them.

```{python}
swiss_histogram = pd.read_csv('Swiss_Production.csv')

# plot distribution of the cost of production, all regions combined.
plt.hist(swiss_histogram['Prix'], bins=50, density=True, alpha=0.5)
plt.xlabel('Price in centimes')
plt.ylabel('Density')
plt.title('Distribution of the cost of production in Switzerland')
plt.show()

```

Regions 5 and 1 are more expensive than other regions. Region 3 has the lowest price in Switzerland.
In order to explain this difference in density between the different regions, we are going to make a boxplot with the five regions.

```{python}
# plot la dispersion des prix pour chaque région, pour potentiellement permettre d'expliquer cet écart de densité.
sns.boxplot(x='Origin', y='Price', data=swiss_production_data)
plt.title('Price distribution by region')
plt.show()
```

Regions 1 and 3 show greater price stability with shorter boxes, indicating less dispersion. Regions 5 and 4 show greater price volatility, as indicated by wider price ranges and the presence of outliers.

### Multivariate Analysis - Switzerland
## Average milk price per system of production over time

```{python}
# plot "l'évolution du prix moyen de production en fonction du système de production"
# Calculer la moyenne des prix pour chaque date et chaque système de production
swiss_production_data_moyenne = swiss_production_data.groupby(['Date', 'Système de production'])['Prix'].mean().reset_index()

# Sélectionner les systèmes de production uniques
systemes_production = swiss_production_data['Système de production'].unique()

# Créer un graphique pour chaque système de production
plt.figure(figsize=(12, 6))  # Ajuste la taille du graphique selon tes besoins

for systeme in systemes_production:

    # Filtrer le DataFrame pour chaque système de production
    swiss_production_data_filtre = swiss_production_data_moyenne[swiss_production_data_moyenne['Système de production'] == systeme]
 
    # Tracer la courbe pour le système de production filtré
    plt.plot(swiss_production_data_filtre['Date'], swiss_production_data_filtre['Prix'],marker='.', linestyle='-', label=systeme)

# Ajout des légendes, titres et étiquettes
plt.title('Évolution du prix moyen par système de production en Suisse')
plt.xlabel('Date')
plt.ylabel('Prix Moyen (centimes de CHF)')
plt.legend(title='Système de production', bbox_to_anchor=(1.05, 1), loc='upper left')  # Ajoute une légende pour distinguer les systèmes et déplace la à l'extérieur
plt.grid(True, which='both', linestyle='--', linewidth=0.5)

plt.xticks(rotation=45)  # Rotation des étiquettes pour les rendre lisibles

plt.tight_layout()  # Ajuste la disposition
plt.show()  # Affiche le graphique
```

The average price per kg of milk produced was higher in early 2001, around 80 cents for conventional milk production and over 95 cents for organic milk. 
The 'unknown' data are highly correlated with the 'conventional' data. 
We note a seasonality in the data, with a non-linear trend: a downward trend between 2001 and 2016 and an upward trend between 2016 and 2024.We'll come back to this later in the report.
Then there is a fall in prices, both organic and conventional, until 2007, followed by a peak in 2008 (a potential correlation with the 2008 crisis?) and a second peak in 2014 for conventional milk. 
From 2020 onwards, prices start to rise again then remain fairly constant over the last two years.


## Average price of milk production, by region and production system

```{python}

swiss_production_data = pd.read_csv('Swiss_Production.csv')
swiss_production_data['Date'] = pd.to_datetime(swiss_production_data['Date']).dt.year
swiss_production_data = swiss_production_data[swiss_production_data['Date'].between(2001, 2024)]

swiss_production_data.sort_values(by=['Provenance du produit', 'Date'], inplace=True)

swiss_production_data['Provenance du produit'] = pd.Categorical(swiss_production_data['Provenance du produit'],
                                                             categories=['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5'],
                                                             ordered=True)

fig = px.bar(
    swiss_production_data, 
    x='Provenance du produit', 
    y='Prix', 
    color='Système de production', 
    barmode='group',
    animation_frame='Date',
     category_orders={"Provenance du Lait": ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5']}
)

fig.update_layout(
    title_text='Average price of milk production, by region and production system',
    xaxis_title='Origin',
    yaxis_title='Average Price in centimes',
)

fig.update_traces(
    hovertemplate='Provenance du produit: %{x}<br>Système de production: %{marker.color}<br>Date: %{animation_frame}<br>Prix:%{y}'
    )
    
years = swiss_production_data['Date'].unique()
steps = []
for year in years:
    step = dict(
        method='animate',
        label=str(year),
        args=[[str(year)], {'frame': {'duration': 300, 'redraw': True}, 'mode': 'immediate'}]
    )
    steps.append(step)

# Add the slider to the figure.
fig.update_layout(
    sliders=[{'steps': steps}]
)

fig.layout.updatemenus[0].buttons = []

fig.show()

```

Regions 5 and 3 have not produced organic milk since 2001. Region 3 also produces less milk on average.


### Swiss Producer
## Description

```{python}
# import data
data = pd.read_csv('../data/Données_marché_Lait.csv')
print('Dataset length:', len(data))
data.head()

# Check the number of unique values in each column
data.nunique()
```
This data set shows the price of organic and conventional milk production from Switzerland and other countries over time.

Source - [Agrimarche](https://www.donnees-agrimarche.ch/donnees)

## Wrangling and Cleaning 

We start by removing the columns with no value or unique value given that they are not relevant for our analysis. 
After proceeding, we end up with a sub-data set of 9 variables.
Then, we calculate the number of missing values for each characteristics and filter out those without.

```{python}
clean_data = data.copy()

# Drop columns with unique values 
clean_data = clean_data.drop(['Devise', 'Groupe de produits', 'Commerce extérieur', 'Indicateur',\
                   'Marché', 'Propriétés du produit', 'Unité', 'Echelon de création de valeur '], axis=1)

# Calculate the number of NaN for each column and filter out those without NaN
nan_counts = clean_data.isna().sum()
nan_counts = nan_counts[nan_counts > 0]
print(nan_counts)

# Create a barplot
plt.figure(figsize=(10, 6))
sns.barplot(x=nan_counts.index, y=nan_counts.values)
plt.title('Nombre de valeurs manquantes par colonne')
plt.xlabel('Colonnes')
plt.ylabel('Nombre de NaN')

plt.show()

# Calculate the percentage of NaN for each column of this cleaned data set 
nan_percent = clean_data.isna().mean() * 100

# Filter to keep only columns with NaN
nan_percent = nan_percent[nan_percent > 0].reset_index()
nan_percent.columns = ['Colonne', 'Pourcentage de NaN']

# Create an interactive graph with Plotly
fig = px.bar(nan_percent, x='Colonne', y='Pourcentage de NaN',
             text='Pourcentage de NaN')

fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')
fig.update_yaxes(range=[0, 100])
fig.update_layout(title_text='Pourcentage de valeurs manquantes par colonne',
                  yaxis_title='Pourcentage de NaN',
                  xaxis_title='Colonne')
                  
#show plot
fig.show()
```
The columns with missing values are: Cost components, Production system, Sales region and Use mode.
The 'Production system' variable has a higher percentage of missing values, around 37.6%. However, this is a key variable in our analysis so we keep it and replace missing information 'NaN' with 'Unknown' for clarity. 
As for the other characteristics, their percentage of missing values is relatively low thus we keep them too.

## General Analysis - All Countries
# Milk Production in Switzerland VS others 

The milk sold in Switzerland comes from 14 different places - 14 values in 'Origin of product'. It would be interesting to see the distribution of the origin of milk in Switzerland, with the percentage of milk coming from Switzerland and the percentage of milk not coming from Switzerland.
We create a new variable 'swiss_production_data' including all five Swiss regions. Then, we group all non-Swiss origins together in another category. This gives us:

```{python}
# create a new variable including all Swiss regions
swiss_production_data = ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5'] 

# Create a new column for classification
clean_data['Provenance'] = clean_data['Provenance du produit'].apply(lambda x: 'Suisse' if x in swiss_production_data else 'Etranger')

# Calculate percentage for both categories
category_counts = clean_data['Provenance'].value_counts(normalize=True) * 100

# Plot the distribution
#use viridis palette colors
plt.figure(figsize=(8, 4))
plt.bar(category_counts.index, category_counts.values, color=['blue', 'red'])
plt.xlabel('Origin')
plt.ylabel('Percentage (%)')
plt.title('Distribution between Swiss and foreign milk')
plt.ylim(0, 100)  
plt.show()
```

The histogram shows a higher percentage of milk comes from Switzerland than from imports. However the difference is small, less than 10%.
This suggests that there are not many Swiss producers in comparison to the population.

# Milk Price by country 

Now that we've compared the distribution of production, we'll compare the distribution of the price of milk by country.

```{python}
# Convert date from string to datetime
clean_data['Date'] = pd.to_datetime(clean_data['Date'])

# Replace the country names with 'Group'
clean_data['Provenance du produit'] = clean_data['Provenance du produit'].replace(swiss_production_data, 'Suisse')

# Group the data by 'Origin of product' & 'Date' and calculate the average price. 
prix_moyen_par_pays_et_annee = clean_data.groupby(['Provenance du produit', 'Date'])['Prix'].mean().reset_index()

# Plot
plt.figure(figsize=(12, 6))
# Loop over each country to trace it
for pays in prix_moyen_par_pays_et_annee['Provenance du produit'].unique():
    subset = prix_moyen_par_pays_et_annee[prix_moyen_par_pays_et_annee['Provenance du produit'] == pays]
    plt.plot(subset['Date'], subset['Prix'], label=pays)
    
plt.legend()
plt.title('Average milk production price by country and year')
plt.xlabel('Year')
plt.ylabel('Average milk production price')
plt.legend(title='Origin', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)

plt.gca().xaxis.set_major_locator(mdates.YearLocator()) 
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y')) 

plt.tight_layout()  
plt.show()
```
We observe from this plot that producing milk in Switzerland is much more expensive than in other countries.

### Univariate Analysis - Switzerland

The main objective of this project is to focus on milk producers in Switzerland and more specifically on their remuneration. 
To do this, we are going to use the variable 'swiss_production_data' we created before, which includes all the Swiss production and sales regions.

## Distribution of milk production for each Swiss region.

```{python}
# Count the occurrences of each type of product
product_counts = swiss_production_data['Provenance du produit'].value_counts()

# Define the desired order of regions
order = ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5']

# Re-index the data in the desired order
product_counts = product_counts.reindex(order)

# Create a bar chart
plt.figure(figsize=(10, 6))
bars = plt.bar(product_counts.index, product_counts.values, color=['red', 'green', 'blue', 'orange', 'purple'])

# Detailed descriptions for each region 
labels = ['Région 1 : Genève, Vaud, Fribourg, Neuchâtel, Jura', 
          'Région 2 : Berne, Lucerne, Unterwald, Uri, Zoug and Schwyz', 
          'Région 3 : Bâle-Campagne & Bâle-Ville, Argovie and Soleure.',
          'Région 4 : Zurich, Schaffhouse, Thurgovie, Appenzell, St.-Gall, Schwyz, Glaris and Grisons',
          'Région 5 : Valais and Tessin.']

for bar, label in zip(bars, labels):
    bar.set_label(label)
    
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))

plt.title('Distribution de la production de lait en Suisse')
plt.xlabel('Provenance du lait')
plt.ylabel('Nombre d’Occurrences')
plt.show()
print(product_counts)
```

We can see that regions 1, 2 and 4 produce a greater quantity of milk than regions 3 and 5.  Créer un tableau affichant les composantes des régions suisses.

## Cost of production 

Given that the sample size differs for each region, we represent the distribution of the cost of production for each region in terms of density to be able to compare them.

```{python}
swiss_histogram = pd.read_csv('Swiss_Production.csv')

# plot distribution of the cost of production, all regions combined.
plt.hist(swiss_histogram['Prix'], bins=50, density=True, alpha=0.5)
plt.xlabel('Price in centimes')
plt.ylabel('Density')
plt.title('Distribution of the cost of production in Switzerland')
plt.show()
```

Regions 5 and 1 are more expensive than other regions. Region 3 has the lowest price in Switzerland.
In order to explain this difference in density between the different regions, we are going to make a boxplot with the five regions.

```{python}
# plot la dispersion des prix pour chaque région, pour potentiellement permettre d'expliquer cet écart de densité.
sns.boxplot(x='Origin', y='Price', data=swiss_production_data)
plt.title('Price distribution by region')
plt.show()
```

Regions 1 and 3 show greater price stability with shorter boxes, indicating less dispersion. Regions 5 and 4 show greater price volatility, as indicated by wider price ranges and the presence of outliers.


### Multivariate Analysis - Switzerland
## Average milk price per system of production over time

```{python}
# plot "Changes in the average production price as a function of the production system"
# Calculate average prices for each date and production system
swiss_production_data_moyenne = swiss_production_data.groupby(['Date', 'Système de production'])['Prix'].mean().reset_index()

# Select unique production systems
systemes_production = swiss_production_data['Système de production'].unique()

# Create a graph for each production system
plt.figure(figsize=(12, 6))

for systeme in systemes_production:
# Filter the DataFrame for each production system
swiss_production_data_filtre = swiss_production_data_moyenne[swiss_production_data_moyenne['Système de production'] == systeme]
# Draw the curve for the filtered production system
plt.plot(swiss_production_data_filtre['Date'], swiss_production_data_filtre['Prix'],marker='.', linestyle='-', label=systeme)

plt.title('Average price trend by production system in Switzerland')
plt.xlabel('Date')
plt.ylabel('Average Price (cents)')
plt.legend(title='System of production', bbox_to_anchor=(1.05, 1), loc='upper left')  # Add a legend 
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.xticks(rotation=45) 

plt.tight_layout()  
plt.show()
```

The average price per kg of milk produced was higher in early 2001, around 80 cents for conventional milk production and over 95 cents for organic milk. 
The 'unknown' data are highly correlated with the 'conventional' data. 
We note a seasonality in the data, with a non-linear trend: a downward trend between 2001 and 2016 and an upward trend between 2016 and 2024. We'll come back to this later in the report.
There was a spike in prices in period 2008, due to the global financial crisis.


## Average price of milk production, by region and production system

```{python}
swiss_production_data = pd.read_csv('Swiss_Production.csv')
swiss_production_data['Date'] = pd.to_datetime(swiss_production_data['Date']).dt.year
swiss_production_data = swiss_production_data[swiss_production_data['Date'].between(2001, 2024)]

swiss_production_data.sort_values(by=['Provenance du produit', 'Date'], inplace=True)

swiss_production_data['Provenance du produit'] = pd.Categorical(swiss_production_data['Provenance du produit'],
                                                             categories=['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5'],
                                                             ordered=True)

fig = px.bar(
    swiss_production_data, 
    x='Provenance du produit', 
    y='Prix', 
    color='Système de production', 
    barmode='group',
    animation_frame='Date',
     category_orders={"Provenance du Lait": ['Région 1', 'Région 2', 'Région 3', 'Région 4', 'Région 5']}
)

fig.update_layout(
    title_text='Average price of milk production, by region and production system',
    xaxis_title='Origin',
    yaxis_title='Average Price in centimes',
)

fig.update_traces(
    hovertemplate='Provenance du produit: %{x}<br>Système de production: %{marker.color}<br>Date: %{animation_frame}<br>Prix:%{y}'
    )
    
years = swiss_production_data['Date'].unique()
steps = []
for year in years:
    step = dict(
        method='animate',
        label=str(year),
        args=[[str(year)], {'frame': {'duration': 300, 'redraw': True}, 'mode': 'immediate'}]
    )
    steps.append(step)

# Add the slider to the figure.
fig.update_layout(
    sliders=[{'steps': steps}]
)

fig.layout.updatemenus[0].buttons = []
fig.show()

```

Regions 5 and 3 have not produced organic milk since 2001. Region 3 is the weakest region on the Swiss milk market, with lower production and a lower price than the other regions. This may be due to the location of the areas within this region.


### Wrangling and Cleaning

```{r}
library(data.table)

file_path <- "../data/"

df_producteur <- read_excel(paste0(file_path, "lait_cru_producteur.xlsx"), sheet = 1)

df_producteur$date <- as.Date(df_producteur$date)
 library(kableExtra)
# Create a tibble with variable descriptions for df_producteur
variable_table <- tibble(
  Variable = c("Date", "prix_bio", "prix_non_bio", "delta", "Delta_pourcent"),
  Description = c(
    "The date when the prices were recorded, in a year-month-day format.",
    "The recorded price of organic milk on the given date.",
    "The recorded price of non-organic milk on the given date.",
    "The absolute difference between the organic and non-organic milk prices.",
    "The percentage difference between the organic and non-organic milk prices."
  )
)

# Display the table using kableExtra
variable_table %>%
  kbl() %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))

```

### Description

This data set explains the Milk price to producers over time. Essential for Macro Analysis of the milk industry in Switzerland.

Have a look :

```{r}
# create a new data cleaned
df_producteur_show <- df_producteur %>%
  mutate(delta = prix_bio - prix_non_bio,
         delta_pourcent = (prix_bio - prix_non_bio) / prix_non_bio * 100) %>%
  select(date, prix_bio, prix_non_bio, delta, delta_pourcent) %>%
  #round all column  to 2 decimal places
  mutate_if(is.numeric, round, 2) 

#print max and min values for delta_pourcent
# max_delta_pourcent <- max(df_producteur_show$delta_pourcent, na.rm = TRUE)
# max_delta_pourcent
# min_delta_pourcent <- min(df_producteur_show$delta_pourcent, na.rm = TRUE)
# min_delta_pourcent

#display cleaned data using reactable
library(reactable)
reactable(
  df_producteur_show,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
```

Source - [asjdaksjdh](https://www.agridea.ch/fr/themes/production-animaliere/bovins/lait-cru/)

## Lait Equitable Sales

### Dataset Sales 2023

#### Wrangling and Cleaning

```{python}
file_path = '../data/all_products_sales_per_stores_2023.xlsx'
df = pd.read_excel(file_path, sheet_name='Sheet1')

df.columns = df.columns.astype(str)
# Renaming the monthly columns for easier readability
new_column_names = {
    '2023-01-01 00:00:00': 'Jan 2023',
    '2023-02-01 00:00:00': 'Feb 2023',
    '2023-03-01 00:00:00': 'Mar 2023',
    '2023-04-01 00:00:00': 'Apr 2023',
    '2023-05-01 00:00:00': 'May 2023',
    '2023-06-01 00:00:00': 'Jun 2023',
    '2023-07-01 00:00:00': 'Jul 2023',
    '2023-08-01 00:00:00': 'Aug 2023',
    '2023-09-01 00:00:00': 'Sep 2023',
    '2023-10-01 00:00:00': 'Oct 2023',
    '2023-11-01 00:00:00': 'Nov 2023',
    '2023-12-01 00:00:00': 'Dec 2023'
}
df.rename(columns=new_column_names, inplace=True)

# Standardize city names based on the mapping provided
correct_city_names = {
    'Bâle': 'Basel',
    'Genève': 'Geneva',
    'Bienne': 'Biel/Bienne',
    'Chavannes': 'Chavannes-de-Bogis',
    'Marin': 'Marin-Epagnier',
    'Vesenaz': 'Vésenaz',
    'Yverdon': 'Yverdon-les-Bains',
    'Saint-Gall Webersbleiche': 'St. Gall'
}
df['Row Labels'] = df['Row Labels'].apply(lambda x: correct_city_names.get(x, x))
```

The dataset from 2023 was meticulously cleaned and standardized to ensure accuracy in our analysis. Initial steps included loading the data from an Excel file and renaming columns to reflect clearer, month-specific sales data for easier readability. Additionally, we corrected city names to maintain consistency across datasets. This included mapping various forms of city names to their standardized counterparts (e.g., 'Bâle' to 'Basel').

#### Description

The dataset is very light and contains monthly sales data for the year 2023. It is essential however for the analysis of the sales of Lait Equitable in different Manor stores.

Here is a preview of the cleaned and structured data:

```{r warning=FALSE, message=FALSE}
#load python df
df_sales_2023 <- py$df

# Load necessary libraries
library(tibble)
library(kableExtra)

# Create a tibble with variable descriptions for df_manor_sales
variable_table <- tibble(
  Variable = c("Row Labels", "Monthly Columns (2023-01-01 to 2023-12-01)", "Grand Total"),
  Description = c(
    "Identifies the Manor store location by name.",
    "Each column represents sales figures for a specific month of 2023",
    "Total sales across all months of 2023 for each location"
  )
)

# Display the table using kableExtra
variable_table %>%
  kbl() %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))

# Using the provided column names correctly in the dataframe df_sales_2023
df_sales_2023_show <- df_sales_2023 %>%
  # Ensure you convert the column names to standard ones if needed
  rename(Location = `Row Labels`) %>%
  # Correctly sum the monthly sales columns from Jan 2023 to Dec 2023
  mutate(Total_Sales = rowSums(select(., `Jan 2023`:`Dec 2023`), na.rm = TRUE)) %>%
  select(Location, `Jan 2023`:`Dec 2023`, Total_Sales) %>%
  mutate_if(is.numeric, round, 2)  # round all numeric columns to 2 decimal places

# Display the data using reactable for an interactive table
reactable(
  df_sales_2023_show,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
```

### Dataset Sales 2022

#### Wrangling and Cleaning

Following the methodology established with the 2023 dataset, the 2022 sales data was similarly processed. The data from 2022, while structurally different, was also standardized to facilitate comparison and analysis. This included renaming columns to ensure uniformity in location names across both datasets.

```{python}
# Load the data for 2022
file_path_2022 = '../data/sales_2022.xlsx'
df_2022 = pd.read_excel(file_path_2022)

# Standardize city names based on the provided mapping
city_name_mapping = {
    'Bâle': 'Basel',
    'Genève': 'Geneva',
    'Bienne': 'Biel/Bienne',
    'Chavannes': 'Chavannes-de-Bogis',
    'Marin': 'Marin-Epagnier',
    'Vesenaz': 'Vésenaz',
    'Yverdon': 'Yverdon-les-Bains',
    'Saint-Gall Webersbleiche': 'St. Gall'
}

# Rename columns to standardize city names
df_2022.rename(columns=city_name_mapping, inplace=True)

# Pivoting the table to get total sales per location for 2022, summing across all products
sales_columns_2022 = [col for col in df_2022.columns if col not in ['Code article', 'Description article', 'Marque', 'Code Fournisseur', 'Description Fournisseur']]
df_2022_total_sales = df_2022[sales_columns_2022].sum().reset_index()
df_2022_total_sales.columns = ['Location', 'Total Sales 2022']
```

#### Description

The 2022 dataset, unlike the 2023 dataset, includes a variety of products, each recorded with sales figures across different locations. This dataset is notably less complex, focusing on total sales rather than monthly breakdowns, yet provides critical insights into the sales performance of different products.

```{r}
# Load the 2022 sales data
df_sales_2022 <- py$df_2022
# Load necessary libraries
library(tibble)
library(kableExtra)

# Create a tibble with variable descriptions for df_sales
variable_table <- tibble(
  Variable = c("Code article", "Description article", "Marque", "Code Fournisseur", "Description Fournisseur",
               "Location Columns (e.g., Ascona-Delta, Baden, Bâle, etc.)"),
  Description = c(
    "Unique identifier for each product.",
    "Descriptive name of the product.",
    "Brand of the product.",
    "Supplier code.",
    "Supplier name.",
    "Each of these columns represents sales figures for that specific location."
  )
)

# Display the table using kableExtra
variable_table %>%
  kbl() %>%
  kable_styling(position = "center", bootstrap_options = c("striped", "bordered", "hover", "condensed"))
```

Here is a closer look at the structured 2022 sales data:

```{r}
library(dplyr)
library(reactable)

# Assuming the dataframe is correctly named df_sales_2022 and is already loaded
# Ensure the dataframe is available in your R environment
print(head(df_sales_2022))

# Prepare the data by calculating the total sales per product across all locations
df_sales_2022_show <- df_sales_2022 %>%
  mutate(Total_Sales = rowSums(select(., `Ascona-Delta`:`Yverdon-les-Bains`), na.rm = TRUE)) %>%
  select(`Code article`, `Description article`, `Marque`, `Code Fournisseur`, `Description Fournisseur`, `Ascona-Delta`:`Yverdon-les-Bains`, Total_Sales) %>%
  mutate_if(is.numeric, round, 2)  # Round all numeric columns to 2 decimal places

# Display the data using reactable for an interactive and visually appealing table
reactable(
  df_sales_2022_show,
  highlight = TRUE,
  defaultPageSize = 5,
  paginationType = "numbers",
  searchable = TRUE,
  sortable = TRUE,
  resizable = TRUE
)

```

#### Merging 2022 and 2023 dataset

```{python}
# Extracting the total sales for 2023 from the first dataset
df_2023_total_sales = df[['Row Labels', 'Grand Total']].rename(columns={'Row Labels': 'Location', 'Grand Total': 'Total Sales 2023'})

# Merging the 2022 and 2023 datasets on Location
merged_sales_data = pd.merge(df_2022_total_sales, df_2023_total_sales, on='Location', how='outer')

# Filling any NaN values that might have occurred due to locations present in one dataset and not the other
merged_sales_data.fillna(0, inplace=True)
```

```{r}
# Load the merged sales data
df_merged_sales <- py$merged_sales_data
#show it using reactable
reactable(
  df_merged_sales,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
```

The 2022 sales data has been aggregated and standardized for each location. The merged dataset now shows the total sales per location for both 2022 and 2023. This dataset offers a comprehensive view of Lait Equitable's sales dynamics over two consecutive years, highlighting trends and changes in consumer behavior across different locations.

### Political Parties Dataset

#### Wrangling and Cleaning

The analysis starts by importing two datasets: sales data (annual sales of fair trade milk) and political party data (support percentages for major parties by location). The political data is cleaned to match commune names in the sales data and transformed into party presence percentages.

Next, the cleaned political data is merged with the sales data based on commune names. This merged dataset enables a combined analysis of political party presence and sales performance.

```{r}
# Read sales data from Excel
sales_data <- read_excel("../data/Ventes annuelles.xlsx")

# Read political party data from Excel
party_data <- read_excel("../data/partisPolitiqueManor.xlsx")

# Clean up party_data to match sales_data locations
party_data_cleaned <- party_data %>%
  mutate(Location = gsub(" ", "", Location)) %>%
  filter(Location %in% sales_data$Location)

# Update party names to match the new column names
party_data_cleaned <- party_data_cleaned %>%
  mutate(PLR_Presence = PLR / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100,
         PS_Presence = PS / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100,
         UDC_Presence = UDC / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100,
         Centre_Presence = Centre / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100,
         Verts_Presence = Verts / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100,
         Vertliberaux_Presence = Vertliberaux / (PLR + PS + UDC + Centre + Verts + Vertliberaux) * 100) %>%
  filter(PLR_Presence > 0.2 | PS_Presence > 0.2 | UDC_Presence > 0.2 | Centre_Presence > 0.2 | Verts_Presence > 0.2 | Vertliberaux_Presence > 0.2)

# Merge sales_data with updated party presence data
merged_data <- merge(sales_data, party_data_cleaned, by = "Location")
```

The analysis starts by importing one other dataset: revenue per capita per commune data. The revenue data is cleaned to match commune names in the sales data.

Next, the cleaned revenue data is merged with the sales data based on commune names. This merged dataset enables a combined analysis of revenue per capita per commune and sales performance.

```{r}
# Load the datasets
revenu_df <- read_excel("../data/revenuParContribuable_CommuneManor.xlsx")

# Merge the datasets on the "Location" column
merged_df <- inner_join(revenu_df, sales_data, by = "Location")

# Clean the data and convert to numeric format
merged_df$`Revenu/contribuable` <- as.numeric(gsub(" ", "", merged_df$`Revenu/contribuable`))
merged_df$`2022` <- as.numeric(gsub(" ", "", merged_df$`2022`))
merged_df$`2023` <- as.numeric(gsub(" ", "", merged_df$`2023`))
```

#### Description

Write kableextra to describe the dataset here

```{r}

```

```{r}
# Display the merged data using reactable
reactable(
  merged_data,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)

```

UTILISER CODE POUR DISPLAY DATA (JAYESH)
