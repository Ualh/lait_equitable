# Exploratory data analysis

EMELINE EDA

## Lait Equitable Products

To provide a comprehensive understanding of Lait Equitable's sales trends throughout 2023, we performed a month-by-month sales analysis. This exploration helps identify seasonal effects, peak sales periods, and potential areas for strategic adjustments. Hereâ€™s a detailed breakdown of the approach and findings:

### Sales Distribution Accross Months

```{r}
# Remove 'Grand Total' column and the row labels column
monthly_sales <- df_sales_2023 %>% 
  select(-c(`Grand Total`, `Row Labels`))

# Aggregate the sales per month across all locations
total_sales_per_month <- colSums(monthly_sales)

# Create a data frame for plotting
monthly_sales_df <- data.frame(Month = names(total_sales_per_month), Sales = total_sales_per_month)

# Sort the data frame by Sales in descending order
sorted_monthly_sales_df <- monthly_sales_df %>%
  arrange(desc(Sales))

# Plotting with ggplot2 using viridis color palette
ggplot(sorted_monthly_sales_df, aes(x=reorder(Month, -Sales), y=Sales, fill=Month)) +
  geom_bar(stat="identity", show.legend = FALSE, fill = "#24918d", color = "black") + 
  labs(title = "Total Sales by Month Across All Locations (2023)", x = "Month", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The graph shows total monthly sales across all locations for the "Lait Equitable" in *2023.* It shows the month with the highest sales, *March* and continuing to lower sales months. The least profitable month appears to be *July.*

-   Highest Sales in March: The graph starts with March, which shows the highest sales, almost reaching 25,000 units. This suggests that March was a particularly strong month for sales, possibly due to seasonal factors or specific marketing campaigns.
-   Gradual Decline in Sales: As we move from left to right, there is a general trend of declining sales. After March, the next highest sales are in December, followed by April, May, and so on. This indicates that sales in March were not sustained throughout the year.
-   Mid-year and End-Year Trends: While the graph is not in chronological order, it shows that some months like December (typically strong due to the holiday season) also performed well, but none reached the peak seen in March.
-   Lower Sales in the Latter Months Displayed: The months at the right end of the graph, such as June and July, show the lowest sales figures in the year. This could indicate a seasonal dip or other market dynamics affecting these months. One supposition could be that people are on vacations at these dates due to school vacations.

### Sales Distribution Accross Locations

```{r}
# First, we need to remove the 'Grand Total' column if it's included
df <- df_sales_2023[, -ncol(df_sales_2023)]
# Sum sales across all months for each location
total_sales_by_location <- df %>%
  mutate(Total_Sales = rowSums(select(., -`Row Labels`))) %>%
  select(`Row Labels`, Total_Sales)

# Sort the locations by total sales in descending order
sorted_sales_by_location <- total_sales_by_location %>%
  arrange(desc(Total_Sales))

# Plotting the data with ggplot2
ggplot(sorted_sales_by_location, aes(x=reorder(`Row Labels`, Total_Sales), y=Total_Sales, fill=`Row Labels`)) +
  geom_bar(stat="identity", show.legend = FALSE,  fill = "#24918d", color = "black") + 
  labs(title = "Total Sales by Location (2023)", x = "Location", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=0.5)) # Rotate the x-axis text for better readability
```

The graph illustrates the total sales by location for Lait Equitable across various stores in 2023, organized from the lowest to the highest sales volume.

-   **Variability in Sales Across Locations:** The graph displays a significant variation in sales across different locations. The left side of the graph shows locations with the least sales, starting with Chur, Rapperswil, St. Gall, and progressively increasing towards the right.
-   **Low Sales in Certain Areas:** Locations like Chur, Rapperswil, and St. Gall have notably low sales, which could indicate either a lower demand for Lait Equitable's products in these areas or possibly less effective marketing and distribution strategies.
-   **High Sales in Specific Locations:** The right end of the graph, particularly the last five locations, shows a sharp increase in sales. Notably, Vevey, Marin-Epagnier, Sierre and Monthey exhibit high sales, with Monthey being the highest. This might indicate a stronger market presence, better consumer acceptance, or more effective promotional activities in these regions.
-   **Potential Market Strengths and Weaknesses:** The graph effectively highlights where Lait Equitable is performing well and where there might be room for improvement. For instance, the high sales in cities like Sierre and Monthey suggest strong market penetration and acceptance.
-   **Strategic Insights:** For the Lait Equitable, this graph provides crucial data points for understanding which locations might need more focused marketing efforts or adjustments in distribution strategies. Additionally, it could help in identifying successful strategies in high-performing locations that could be replicated in areas with lower sales.

```{r}
#remove grand total
df <- df_sales_2023[, -ncol(df_sales_2023)]
# Transform the data into a long format where each row contains a location, a month, and sales
long_data <- df %>%
  pivot_longer(cols = -`Row Labels`, names_to = "Month", values_to = "Sales") %>%
  mutate(Location = `Row Labels`)

# Create a plotly object for an interactive boxplot
fig <- plot_ly(long_data, x = ~Location, y = ~Sales, type = 'box',
               hoverinfo = 'text', text = ~paste('Month:', Month, '<br>Sales:', Sales),
               marker = list(color = "#7e57c2",
                             boxpoints = "all",
                             jitter = 0.3),
               box = list(line = list(color = "#24918d"))
               ) %>% 
  layout(title = "Distribution of Monthly Sales Across Locations",
         xaxis = list(title = "Location"),
         yaxis = list(title = "Monthly Sales"),
         showlegend = FALSE, 
         width= 600,
         height = 800) %>% 
  config(displayModeBar = FALSE) # Optional: hide the mode bar


# Display the plot
fig

```

This graphs show the variability in sales across different locations for each month in 2023. The boxplot provides a visual representation of the distribution of sales figures, highlighting the range, median, and outliers in each location.

We observe that the outliers are high or low sales months as we analyze previously. It confirms the previous analysis and provides a more detailed view of the sales distribution across locations.

### Top Performing / Worse Performing Locations

```{r}
#using grand total to sort the data from top to bottom
df <- df_sales_2023
#using 'grand total' column as total sales plot the top and bottom locations
df %>%
  arrange(desc(`Grand Total`)) %>%
  slice_head(n = 5) %>%
  select(`Row Labels`, `Grand Total`) %>%
  ggplot(aes(x = reorder(`Row Labels`, `Grand Total`), y = `Grand Total`, fill = `Row Labels`)) +
  geom_bar(stat = "identity", show.legend = FALSE, fill = "#33848D", color = "black") +
  labs(title = "Top 5 Performing Locations by Total Sales (2023)", x = "Location", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=0.5)) # Rotate the x-axis text for better readability

# worse performing locations
df %>%
  arrange(`Grand Total`) %>%
  slice_head(n = 5) %>%
  select(`Row Labels`, `Grand Total`) %>%
  ggplot(aes(x = reorder(`Row Labels`, `Grand Total`), y = `Grand Total`, fill = `Row Labels`)) +
  geom_bar(stat = "identity", show.legend = FALSE, fill = "#33848D", color = "black") +
  labs(title = "Bottom 5 Performing Locations by Total Sales (2023)", x = "Location", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=0.5)) # Rotate the x-axis text for better readability

```

As previously analyzed, the top and bottom performing locations are displayed in the bar charts. The top 5 locations with the highest total sales are shown in the first graph, while the bottom 5 locations with the lowest total sales are displayed in the second graph.

Top-performing locations are : Monthey, Sierre, Marin-Epagnier, Vevey, and Sion. Worse-performing locations are : Basel, St. Gall, Sargans, Rapperswil, and Chur.

### 2022 vs 2023

```{r}
#plot a bar chart to compare the total sales in 2022 and 2023 and add transparency to the bars
df_merged_sales %>%
  ggplot(aes(x = reorder(Location, -`Total Sales 2023`), y = `Total Sales 2023`, fill = "2023")) +
  geom_bar(aes(x = reorder(Location, -`Total Sales 2022`), y = `Total Sales 2022`, fill = "2022"), stat = "identity", position = "dodge", fill = "#7e57c2", color = "black", alpha = 0.7 ) +
  geom_bar(stat = "identity", position = "dodge", fill = "#33848D", color = "black", alpha = 0.7) +
  labs(title = "Total Sales Comparison Between 2022 and 2023 by Location", x = "Location", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust=0.5)) # Rotate the x-axis text for better readability

```

-   **Trend of Decline**: A significant number of Manor locations have lower sales figures in 2023 compared to 2022. This trend suggests that Lait Equitable might be facing challenges in these areas, which could include increased competition, changing consumer preferences, or other market dynamics affecting the demand for their products.

-   **Monthey's Decline**: The bar chart shows that Monthey experienced a substantial decrease in sales in 2023 compared to 2022. This would be a key point of concern for Lait Equitable, and understanding why Monthey is underperforming is essential. This could be due to a range of factors, such as local economic conditions, operational challenges, increased competition, or changes in consumer preference within that particular area.

### Map
```{python producer_map}
import re
locations = [
      "46.5907889,6.7386114","46.5842792,6.6649234","46.5149217,6.888093","46.5366608,6.7054361","46.2962374,6.9946778","46.5575317,6.7491415","46.5407979,6.7757944","46.5732277,6.7334704","46.6179705,6.6729034","46.4438248,6.9216581","46.5244929,6.7652973","46.5535214,6.7766663","46.9087674,7.0772625","46.5072978,6.7303043","46.7658972,6.5635194","46.4890932,6.839986","46.5343528,6.7394634","46.6558216,6.7369549","46.5156779,6.8621498","46.6112911,6.8073595","46.6666946,6.8991534","46.7395322,6.9715464","46.6468967,6.9540885","46.5153462,6.8584537","46.5975359,7.0382028","46.595883,6.8855935","46.6612814,6.8782794","46.7087666,7.1239746","46.6510812,6.9711981","46.6141324,7.0318659","46.601508,7.1001934","46.6091436,7.0307497","46.5766179,6.8634981","46.5954987,6.9855279","46.829064,6.9264143","46.6588489,6.8758917","46.6124961,6.9757718","47.2915521,7.3217577","47.2775379,7.3830611","47.1202632,6.8834564","47.2786229,7.3611541","47.270679,7.39172","47.3439771,7.4949095","47.3276332,7.218765","47.3660465,7.4060015","47.3649491,7.46977","47.2032746,6.9890961","47.2092332,7.0068912","46.3260628,6.9042257","46.2126721,6.9985419","46.1163846,7.112862","46.3649142,6.8995776","46.9232972,6.4673148","47.0769951,6.756922","46.0377232,8.8837298","46.1646151,8.9659276","46.1292513,8.985035","46.0029796,8.8540675","46.4694663,8.9413945","46.1460954,8.9345517","47.3859362,7.4313549","47.4475168,7.5593599","47.3809253,7.4221544","47.4364108,9.3341963","47.4304657,9.3321608","47.4341264,9.3481821","47.4345287,9.3262813","47.426417,9.3338866","47.4652773,9.1518866","47.2749047,9.5092842","47.1135124,9.1297439","47.404214,9.3419733","47.5295845,8.4670763","47.1738126,8.3342979","47.08052,8.2031428","47.2351478,8.1477158","46.7883954,7.4004079","47.0364909,7.2786631","46.8364949,7.7665098","46.7833635,7.3912478"]
```

```{python map}
# Function to calculate the dynamic radius
def calculate_radius(volume, max_volume, min_volume, max_radius=20):
    normalized_volume = (volume - min_volume) / (max_volume - min_volume)
    return normalized_volume * max_radius + 3

# Function to get latitude and longitude
def get_lat_lon(city):
    try:
        time.sleep(1)  # Simple rate-limiting mechanism
        location = geolocator.geocode(city + ', Switzerland')
        return location.latitude, location.longitude
    except AttributeError:
        return None, None

# Read data from different product categories
file_paths = {
    'All Products': ("../data/Produits laitiers Ã©quitables - 2023.xlsb", 'Par SM'),
    'Milk Drink': ("../data/lait_drink_sales_per_stores_2023.xlsx", 'Sheet1'),
    'Milk Entier': ("../data/lait_entier_sales_per_stores_2023.xlsx", 'Sheet1'),
    'Fondue': ("../data/fondue_sales_per_stores_2023.xlsx", 'Sheet1'),
    'Delice': ("../data/delice_sales_per_stores_2023.xlsx", 'Sheet1'),
    'Creme': ("../data/creme_cafe_sales_per_stores_2023.xlsx", 'Sheet1')
}

# Create a folium map
m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)
# Instantiate the geolocator
geolocator = Nominatim(user_agent="le_stores")

#####map for store locations per products
# Loop through each category 
for category, (file_path, sheet_name) in file_paths.items():
    engine = 'pyxlsb' if 'xlsb' in file_path else None
    df = pd.read_excel(file_path, engine=engine, sheet_name=sheet_name)

    if category == 'All Products':
        # Skip the first six rows and rename columns based on the provided structure
        df = df.iloc[6:]  
        df.rename(columns={
            'QuantitÃ©s vendues - annÃ©e 2023': 'City',
            'Unnamed: 1': '01/01/2023',
            'Unnamed: 2': '02/01/2023',
            'Unnamed: 3': '03/01/2023',
            'Unnamed: 4': '04/01/2023',
            'Unnamed: 5': '05/01/2023',
            'Unnamed: 6': '06/01/2023',
            'Unnamed: 7': '07/01/2023',
            'Unnamed: 8': '08/01/2023',
            'Unnamed: 9': '09/01/2023',
            'Unnamed: 10': '10/01/2023',
            'Unnamed: 11': '11/01/2023',
            'Unnamed: 12': '12/01/2023',
            'Unnamed: 13': 'Total General'
        }, inplace=True)
    else:
        # Renaming columns for XLSX files based on your last dataframe example
        df.rename(columns={
            df.columns[0]: 'City',
            df.columns[-1]: 'Total General'
        }, inplace=True)

    # Standardize city names
    correct_city_names = {
        'BÃ¢le': 'Basel',
        'GenÃ¨ve': 'Geneva',
        'Bienne': 'Biel/Bienne',
        'Chavannes': 'Chavannes-de-Bogis',
        'Marin': 'Marin-Epagnier',
        'Vesenaz': 'VÃ©senaz',
        'Yverdon': 'Yverdon-les-Bains',
        'Saint-Gall Webersbleiche': 'St. Gall'
    }
    df['City'] = df['City'].apply(lambda x: correct_city_names.get(x, x))

    # Get latitudes and longitudes
    df[['Lat', 'Lon']] = df.apply(lambda row: pd.Series(get_lat_lon(row['City'])), axis=1)

    # Define color scale and feature group
    max_sales = df['Total General'].max()
    min_sales = df['Total General'].min()
    color_scale = cmp.linear.viridis.scale(min_sales, max_sales)
    fg = folium.FeatureGroup(name=category)
    
    # Add markers
    for index, row in df.iterrows():
        if pd.notnull(row['Lat']) and pd.notnull(row['Lon']):
            radius = calculate_radius(row['Total General'], max_sales, min_sales)
            folium.CircleMarker(
                location=[row['Lat'], row['Lon']],
                radius=radius,
                popup=f"{row['City']}: {row['Total General']}",
                color=color_scale(row['Total General']),
                fill=True,
                fill_color=color_scale(row['Total General'])
            ).add_to(fg)

    fg.add_to(m)
###heatmap of producers
heat_data = [[float(lat), float(lon)] for loc in locations for lat, lon in [loc.split(',')]]

# Add HeatMap layer
HeatMap(heat_data).add_to(m)

# Add layer control and save the map
folium.LayerControl().add_to(m)
m.save('combined_product_map.html')
m
```

#### Distance Matrix
```{python}
from geopy.distance import geodesic
latitudes = []
longitudes = []

# Parse each location and extract latitude and longitude
for location in locations:
    lat, lon = location.split(',')
    latitudes.append(float(lat))
    longitudes.append(float(lon))

# Create a DataFrame using pandas
producers = pd.DataFrame({
    'Latitude': latitudes,
    'Longitude': longitudes
})

#get the df from the python code
cities = df['City']

# Initialize an empty DataFrame to store distances
distance_matrix = pd.DataFrame(index=cities, columns=[f"Producer {i+1}" for i in range(len(producers))])

# Calculate distances and fill the DataFrame
for city in cities:
    city_lat, city_lon = get_lat_lon(city)
    if city_lat is not None and city_lon is not None:
        city_coords = (city_lat, city_lon)
        for index, producer in producers.iterrows():
            producer_coords = (producer['Latitude'], producer['Longitude'])
            distance = geodesic(city_coords, producer_coords).kilometers  # distance in kilometers
            distance_matrix.loc[city, f"Producer {index+1}"] = distance
            

# Flatten the DataFrame to get all distance values in one series
all_distances = distance_matrix.values.flatten()

# Basic statistics
print("Distance Statistics:")
print("Minimum Distance: {:.2f} km".format(all_distances.min()))
print("Maximum Distance: {:.2f} km".format(all_distances.max()))
print("Average Distance: {:.2f} km".format(all_distances.mean()))
print("Median Distance: {:.2f} km".format(np.median(all_distances)))

# Histogram of the distances
plt.figure(figsize=(10, 6))
plt.hist(all_distances, bins=30, color='#24918d', alpha=0.7)
plt.title('Distribution of Distances Between Cities and Producers')
plt.xlabel('Distance in km')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```



```{python}
df_sales = r.get('df_merged_sales')
#rename 'Location' column as 'City'
df_sales.rename(columns={'Location': 'City'}, inplace=True)
df_sales.set_index('City', inplace=True)

# Calculate the minimum distance for each city and add it to df_sales
df_sales['Min Distance to Producer'] = distance_matrix.min(axis=1)

# Calculate Pearson correlation
correlation_2022 = df_sales[['Total Sales 2022', 'Min Distance to Producer']].corr(method='pearson')
correlation_2023 = df_sales[['Total Sales 2023', 'Min Distance to Producer']].corr(method='pearson')

print("Correlation between Total Sales 2022 and Min Distance to Producer:")
print(correlation_2022)

print("Correlation between Total Sales 2023 and Min Distance to Producer:")
print(correlation_2023)

# Convert 'Total Sales 2022' and 'Min Distance to Producer' to numeric types explicitly
df_sales['Total Sales 2022'] = pd.to_numeric(df_sales['Total Sales 2022'], errors='coerce')
df_sales['Min Distance to Producer'] = pd.to_numeric(df_sales['Min Distance to Producer'], errors='coerce')

# Recheck data types
print(df_sales.dtypes)

# Plotting Total Sales 2022 vs. Min Distance to Producer
plt.figure(figsize=(10, 6))
sns.regplot(
    x='Min Distance to Producer', 
    y='Total Sales 2022', 
    data=df_sales,
    scatter_kws={'s': 50, 'color': 'red'},  # Customizing the scatter plot points
    line_kws={'color': 'blue', 'lw': 2}  # Customizing the regression line
)
plt.title('Total Sales 2022 vs. Minimum Distance to Producer')
plt.xlabel('Minimum Distance to Producer (km)')
plt.ylabel('Total Sales 2022')
plt.grid(True)
plt.show()

# Plot for 2023
plt.figure(figsize=(10, 6))
sns.regplot(x='Min Distance to Producer', y='Total Sales 2023', data=df_sales, 
            scatter_kws={'s': 50, 'color': 'red'}, line_kws={'color': 'blue'})
plt.title('Total Sales 2023 vs. Min Distance to Producer')
plt.xlabel('Minimum Distance to Producer (km)')
plt.ylabel('Total Sales 2023')
plt.grid(True)
plt.show()
```


1. Comparing Producers Two by Two for Each City: This involves iterating over each column (city) and comparing every pair of producers to determine which one is closer, while ensuring the distance is less than 30 km. If neither producer in the comparison meets the distance condition, the count for that pair for that city is not increased.
2. Counting the Assignments: For each city, track how many times each producer was the closest among the pairs compared, given they are within 30 km. This is a cumulative count across all cities.
3. Merging the Count Data with Sales Data: Merge the proximity count with sales data for correlation analysis.
```{python}
# Initialize a DataFrame to store scores for each producer
scores = pd.DataFrame(0, index=distance_matrix.index, columns=['Score'])

# Iterate through each city (column) to find the producer with the minimum distance
for city in distance_matrix.columns:
    min_distance_producer = distance_matrix[city].idxmin()
    scores.loc[min_distance_producer, 'Score'] += 1

#merges scores with df_sales
df = df_sales.merge(scores, left_index=True, right_index=True, how='left')
df
correlation_matrix = df[['Total Sales 2022', 'Total Sales 2023', 'Score']].corr()

# Display the correlation matrix
correlation_matrix
```

```{python}
# Plotting 'Sales 2022' vs 'Score'
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
sns.regplot(x='Total Sales 2022', y='Score', data=df, ci=None, scatter_kws={'color': 'red'}, line_kws={'color': 'blue'})
plt.title('Sales 2022 vs. Score')

# Plotting 'Sales 2023' vs 'Score'
plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
sns.regplot(x='Total Sales 2023', y='Score', data=df, ci=None, scatter_kws={'color': 'green'}, line_kws={'color': 'blue'})
plt.title('Sales 2023 vs. Score')

# Show the plots
plt.tight_layout()
plt.show()

```

```{python}
# Count the frequency of each city being the minimum
city_counts = pd.Series(min_cities).value_counts()

# Merge the counts with the sales data
result_df = pd.DataFrame({
    'City': city_counts.index,
    'Producers Close': city_counts.values
})

result_df = result_df.merge(df_sales, left_on='City', right_index=True, how='left')

# Now you have a DataFrame `result_df` with city, number of times it's chosen as minimum, and sales
result_df
```

```{python}
#create a interactive plot for checking the outliers
fig = px.scatter(df_sales, x='Min Distance to Producer', y='Total Sales 2023', color='Total Sales 2023', hover_name=df_sales.index)
fig.update_layout(title='Total Sales 2023 vs. Minimum Distance to Producer', xaxis_title='Minimum Distance to Producer (km)', yaxis_title='Total Sales 2023')
fig.show()
```

```{python}
```

```{python}
```


Distance Statistics:
Minimum Distance: 0.68 km
Maximum Distance: 283.46 km
Average Distance: 116.73 km
Median Distance: 117.62 km

## Price To Producers Lait Cru

### Organic Milk vs Non Organic (bio) Milk

```{r}
# Create xts object
prices_xts <- xts(df_producteur[, c("prix_bio", "prix_non_bio")], order.by = df_producteur$date)

# Plot using dygraphs
dygraph(prices_xts, main = "Trends in Milk Prices (Organic vs. Non-Organic)", width = "600px", height = "400px") %>%
  dySeries("prix_bio", label = "Organic Price", color = "#24918d") %>%
  dySeries("prix_non_bio", label = "Non-Organic Price", color = "#7e57c2") %>%
  dyOptions(stackedGraph = FALSE) %>%
  dyRangeSelector(height = 20)


# Create an xts object for the delta series, ensuring the series name is retained
delta_xts <- xts(x = df_producteur[,"delta", drop = FALSE], order.by = df_producteur$date)

# Plot using dygraphsdf_
p_delta <- dygraph(delta_xts, main = "Difference in Prices Between Organic and Non-Organic Milk Over Time", width = "600px", height = "400px") %>%
  dySeries("delta", label = "Delta in Price", color = "#24918d") %>%
  dyOptions(stackedGraph = FALSE) %>%
  dyRangeSelector(height = 20)

# Print the dygraph to display it
p_delta
```

### Seasonality

```{r}
# Process the data to extract month and year
df_producteur <- df_producteur %>%
  mutate(Month = format(date, "%m"),
         Year = format(date, "%Y")) %>%
  arrange(date) # Ensure data is in chronological order

# Plotting the data with ggplot2, showing the trend within each year
p_seaso_2 <- ggplot(df_producteur, aes(x = Month, y = prix_bio, group = Year, color = as.factor(Year))) +
  geom_smooth(se = FALSE, method = "loess", span = 0.3, size = 0.7) +
  labs(title = "Monthly Milk Prices by Year",
       x = "Month",
       y = "Price of Organic Milk",
       color = "Year") +
  theme_minimal() +
  scale_color_viridis_d() +
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 45, hjust = 1))

# Convert to an interactive plotly object
interactive_plot_seaso_2 <- ggplotly(p_seaso_2, width = 600, height = 400)

# Adjust plotly settings 
interactive_plot_seaso_2 <- interactive_plot_seaso_2 %>%
  layout(margin = list(l = 40, r = 10, b = 40, t = 40), # Adjust margins
         legend = list(orientation = "h", x = 0, xanchor = "left", y = -0.2)) # Adjust legend position

# Display the interactive plot
interactive_plot_seaso_2
```
