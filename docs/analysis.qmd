# Analysis

-   Answers to the research questions
-   Different methods considered
-   Competing approaches
-   Justifications

## Forecasting Next Year Milk Prices

```{r}
# re-arragen the df_producteur data in ascending order
df_producteur <- df_producteur[order(df_producteur$date),]

#creating tsibble for organic and non-organic milk prices
df_producteur_ts_non_bio <- ts(df_producteur$prix_non_bio, start=c(2017, 12), frequency=12)
df_producteur_ts_bio <- ts(df_producteur$prix_bio, start=c(2017, 12), frequency=12)

#convert the ts object to a tsiible object
df_producteur_ts_non_bio <- as_tsibble(df_producteur_ts_non_bio)
df_producteur_ts_bio <- as_tsibble(df_producteur_ts_bio)
```

### Naive Forecast

```{r}
# Fit a naive model
fit_non_bio <- df_producteur_ts_non_bio %>% model(naive = NAIVE(value))
fit_bio <- df_producteur_ts_bio %>% model(naive = NAIVE(value))

# Forecast the next 12 months
naive_forecast_non_bio <- fit_non_bio %>% forecast(h = 12)
naive_forecast_bio <- fit_bio %>% forecast(h = 12)

plot <- naive_forecast_non_bio %>%
  autoplot(df_producteur_ts_non_bio, alpha = 0.5) +
  labs(title = "Naive Forecast of Non-Organic Milk Prices",
       x = "Date",
       y = "Price") + guides(colour = guide_legend(title = "Forecast"))
plot
plot <- naive_forecast_bio %>%
  autoplot(df_producteur_ts_bio, alpha = 0.5) +
  labs(title = "Naive Forecast of Non-Organic Milk Prices",
       x = "Date",
       y = "Price") + guides(colour = guide_legend(title = "Forecast"))
plot
```

We observe that this model is very vague because is just a naive model that assumes that the next value will be the same as the last value. But it gives us a starting point to compare with other models.

### ARIMA Model

#### Stationarity

```{r}
# re-arragen the df_producteur data in ascending order
df_producteur <- df_producteur[order(df_producteur$date),]

#creating tsibble for organic and non-organic milk prices
df_producteur_ts_non_bio <- ts(df_producteur$prix_non_bio, start=c(2017, 12), frequency=12)
df_producteur_ts_bio <- ts(df_producteur$prix_bio, start=c(2017, 12), frequency=12)
#check for stationarity
adf.test(df_producteur_ts_non_bio)
adf.test(df_producteur_ts_bio)
```

We analyse the stationarity of the time series data for both organic and non-organic milk prices using the Augmented Dickey-Fuller (ADF) test. The Augmented Dickey-Fuller (ADF) test is commonly used to determine whether a unit root is present in a time series dataset. A unit root suggests that a time series is non-stationary, meaning its statistical properties such as mean and variance change over time. On the other hand, if the null hypothesis of the ADF test is rejected, it indicates that the time series is stationary.

We do that because ARIMA models require the time series data to be stationary.

The results of the ADF test for both time series `df_producteur_ts_non_bio` and `df_producteur_ts_bio` indicate:

-   Dickey-Fuller statistic value of -3
-   Lag order of 4
-   p-value of 0.08

Solely based on the p-values provided (0.08), we cannot conclusively determine whether the time series data df_producteur_ts_non_bio and df_producteur_ts_bio are stationary or not. They might be stationary, but further analysis or additional tests might be needed for a more definitive conclusion.

We can thus differenciate the data to make it stationary.

```{r}
#difference the time series
df_producteur_ts_non_bio_diff <- diff(df_producteur_ts_non_bio)
df_producteur_ts_bio_diff <- diff(df_producteur_ts_bio)

#plot them to see the differentiation
autoplot(df_producteur_ts_non_bio_diff)+ labs(title = "Differenced Time Series of Organic Milk Prices")
autoplot(df_producteur_ts_bio_diff) + labs(title = "Differenced Time Series of Bio Milk Prices")

#check for stationarity
adf.test(df_producteur_ts_non_bio_diff)
adf.test(df_producteur_ts_bio_diff)
```

The results of the ADF test for both differenced time series indicate:

-   Dickey-Fuller statistic value of -6
-   Lag order of 4
-   p-value of 0.01

In this case, the p-value is smaller than the typical significance level of 0.05, indicating strong evidence against the null hypothesis. Therefore, based on the p-values provided (0.01), we can conclude that the differenced time series data are likely stationary.

This suggests that after differencing, the time series data df_producteur_ts_non_bio and df_producteur_ts_bio have become stationary, which is often desirable for various time series analysis techniques and forecasting models.

#### Fitting the ARIMA Model and Forecasting

```{r}
# Fit the ARIMA model
fit_non_bio <- auto.arima(df_producteur_ts_non_bio, seasonal = FALSE)
fit_bio <- auto.arima(df_producteur_ts_bio, seasonal = FALSE)

# Forecast the next 12 months
forecast_non_bio <- forecast(fit_non_bio, h = 12)
forecast_bio <- forecast(fit_bio, h = 12)

#show the components used for the ARIMA model
fit_non_bio %>% summary()
fit_bio %>% summary()

#plot the forecasted values
autoplot(forecast_non_bio) + labs(title = "Forecasted Prices of Non-Organic Milk")
```

```{r}
autoplot(forecast_bio) + labs(title = "Forecasted Prices of Organic Milk")
```

#### Fit a SARIMA Model

```{r}
# Fit the SARIMA model
fit_non_bio_sarima <- auto.arima(df_producteur_ts_non_bio, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
fit_bio_sarima <- auto.arima(df_producteur_ts_bio, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)

# Forecast the next 12 months
forecast_non_bio_sarima <- forecast(fit_non_bio_sarima, h = 12)
forecast_bio_sarima <- forecast(fit_bio_sarima, h = 12)

#plot the forecasted values
autoplot(forecast_non_bio_sarima) + labs(title = "Forecasted Prices of Non-Organic Milk (SARIMA)")
autoplot(forecast_bio_sarima) + labs(title = "Forecasted Prices of Organic Milk (SARIMA)")
```

#### Compare ARIMA and SARIMA forecast

##### Organic Milk SARIMA vs ARIMA

```{r}
# compare forecast_bio vs forecast_bio_sarima Model using AIC

```

##### Non-Organic Milk SARIMA vs ARIMA

```{r}
# compare forecast_non_bio vs forecast_non_bio_sarima using AIC

```

#### Forecasted Prices ARIMA

```{r}
# Create a table of the forecasted values 
forecast_table_arima <- tibble(
  Month = seq(as.Date("2023-01-01"), by = "month", length.out = 12),
  Non_Organic_Forecast = forecast_non_bio$mean,
  Bio_Forecast = forecast_bio$mean
)
#round the forecasted values
forecast_table_arima <- forecast_table_arima %>%
  mutate(across(c(Non_Organic_Forecast, Bio_Forecast), ~round(., 2)))
#show the forecasted values using reactable
reactable(
  forecast_table_arima,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
#plot the forecasted values
forecast_table_arima %>%
  pivot_longer(cols = c(Non_Organic_Forecast, Bio_Forecast), names_to = "Type", values_to = "Forecasted_Price") %>%
  ggplot(aes(x = Month, y = Forecasted_Price, color = Type)) +
  geom_line() +
  labs(title = "Forecasted Prices of Organic and Non-Organic Milk",
       x = "Month",
       y = "Price",
       color = "Type") +
  theme_minimal()
```

We used the mean values of the forecasted prices for both organic and non-organic milk to create a table and plot the forecasted prices for the next 12 months. The table provides a detailed view of the forecasted prices, while the plot visualizes the trend of the forecasted prices over time.

#### Forecasted Prices SARIMA

```{r}
# Create a table of the forecasted values 
forecast_table_sarima <- tibble(
  Month = seq(as.Date("2023-01-01"), by = "month", length.out = 12),
  Non_Organic_Forecast = forecast_non_bio_sarima$mean,
  Bio_Forecast = forecast_bio_sarima$mean
)
#round the forecasted values
forecast_table_sarima <- forecast_table_sarima %>%
  mutate(across(c(Non_Organic_Forecast, Bio_Forecast), ~round(., 2)))
#show the forecasted values using reactable
reactable(
  forecast_table_sarima,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
#plot the forecasted values
forecast_table_sarima %>%
  pivot_longer(cols = c(Non_Organic_Forecast, Bio_Forecast), names_to = "Type", values_to = "Forecasted_Price") %>%
  ggplot(aes(x = Month, y = Forecasted_Price, color = Type)) +
  geom_line() +
  labs(title = "Forecasted Prices of Organic and Non-Organic Milk",
       x = "Month",
       y = "Price",
       color = "Type") +
  theme_minimal()
```

### Exponential Smoothing

```{r}
# Fit the ETS model
fit_non_bio_ets <- ets(df_producteur_ts_non_bio)
fit_bio_ets <- ets(df_producteur_ts_bio)

# Forecast the next 12 months
forecast_non_bio_ets <- forecast(fit_non_bio_ets, h = 12)
forecast_bio_ets <- forecast(fit_bio_ets, h = 12)

#plot the forecasted values
autoplot(forecast_non_bio_ets) + labs(title = "Forecasted Prices of Non-Organic Milk (ETS)")
autoplot(forecast_bio_ets) + labs(title = "Forecasted Prices of Organic Milk (ETS)")
```

```{r}
# Create a table of the forecasted values
forecast_table_ets <- tibble(
  Month = seq(as.Date("2023-01-01"), by = "month", length.out = 12),
  Non_Organic_Forecast_ETS = forecast_non_bio_ets$mean,
  Bio_Forecast_ETS = forecast_bio_ets$mean
)
forecast_table_ets

#plot the forecasted values
forecast_table_ets %>%
  pivot_longer(cols = c(Non_Organic_Forecast_ETS, Bio_Forecast_ETS), names_to = "Type", values_to = "Forecasted_Price") %>%
  ggplot(aes(x = Month, y = Forecasted_Price, color = Type)) +
  geom_line() +
  labs(title = "Forecasted Prices of Organic and Non-Organic Milk (ETS)",
       x = "Month",
       y = "Price",
       color = "Type") +
  theme_minimal()
```

```{r}
# compare ARIMA and ETS forecast

```

## Lait Equitable Analysis

### Pareto Principle

The Pareto Principle, often known as the 80/20 rule, asserts that a small proportion of causes, inputs, or efforts usually lead to a majority of the results, outputs, or rewards. Applied to a business context where approximately 20% of the sales account for 80% of the revenues, this principle can help in identifying and focusing on the most profitable aspects of a business.

Evidence from Research:

Sales and Customer Concentration: Research has consistently shown that a significant portion of sales often comes from a minority of customers or products. For instance, an analysis across 22 different consumer packaged goods categories found an average Pareto ratio (PR) of .73, indicating that the top proportion of products/customers often account for a disproportionately high share of sales or profits Source - [Kim, Singh, & Winer, 2017](https://doi.org/10.1007/S11002-017-9442-5)

Decision Making and Resource Allocation: The Pareto Principle helps in decision-making by highlighting areas where the greatest impact can be achieved. For example, focusing on the top-performing products or customers can optimize resource allocation and maximize profits Source - [Ivančić, 2014](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2841896)

Market and Profit Concentration: Another study noted that a small number of customers are often responsible for a large portion of sales, which supports the strategic focus on these customers to boost profitability and efficiency Source- [McCarthy & Winer, 2018](https://doi.org/10.2139/ssrn.3264425)

Conclusion: Applying the Pareto Principle in a business context where a minority of sales drives the majority of revenue can lead to more focused and effective business strategies, optimizing efforts towards the most profitable segments. This approach not only simplifies decision-making but also enhances resource allocation, ultimately leading to increased profitability.

#### Steps

1.  Calculating the total sales across all locations for both 2022 and 2023.
2.  Ranking locations by sales to see the cumulative contribution of each location towards the total.
3.  Identifying the point where approximately 20% of the locations contribute to around 80% of the sales.

```{python}
# Calculate the total sales for each year and the combined total to apply Pareto Principle
merged_sales_data['Combined Sales'] = merged_sales_data['Total Sales 2022'] + merged_sales_data['Total Sales 2023']

# Sort locations by combined sales
pareto_data = merged_sales_data.sort_values(by='Combined Sales', ascending=False)

# Calculate cumulative sales
pareto_data['Cumulative Sales'] = pareto_data['Combined Sales'].cumsum()

# Calculate the total of combined sales
total_combined_sales = pareto_data['Combined Sales'].sum()

# Calculate the percentage of cumulative sales
pareto_data['Cumulative Percentage'] = 100 * pareto_data['Cumulative Sales'] / total_combined_sales

# Find the point where about 20% of the locations contribute to approximately 80% of the sales
pareto_data['Location Count'] = range(1, len(pareto_data) + 1)
pareto_data['Location Percentage'] = 100 * pareto_data['Location Count'] / len(pareto_data)

# Plotting the Pareto curve
plt.figure(figsize=(12, 8))
cumulative_line = plt.plot(pareto_data['Location Percentage'], pareto_data['Cumulative Percentage'], label='Cumulative Percentage of Sales', color='b', marker='o')
plt.axhline(80.2, color='r', linestyle='dashed', linewidth=1)
plt.axvline(33.3, color='green', linestyle='dashed', linewidth=1)
plt.title('Pareto Analysis of Sales Across Locations')
plt.xlabel('Cumulative Percentage of Locations')
plt.ylabel('Cumulative Percentage of Sales')
plt.legend()
plt.grid(True)
plt.show()
```

Given this graph 33.2% of Manor locations are contributing to 80% of sales. This deviates from the typical Pareto 80/20 distribution, but it still shows a concentration of sales among a subset of stores.

#### Observations

We will identify the top 33.3% of locations based on their cumulative sales contribution. This means selecting the smallest number of locations that together account for at least 80% of the total sales.

The top-performing 33.3% of Manor locations that contribute to the majority of sales are:

```{python}
# Calculate the threshold for the top 33.3% of locations
top_third_index = int(len(pareto_data) * 0.34)

# Identifying the top 33.3% of stores contributing to at least 80% of sales
top_performing_stores = pareto_data.head(top_third_index)
top_performing_stores
```

### Understanding Success Factors of Top-Performing Stores

#### Correlating Political Parties with Milk Sales

Here, we will then make a scatterplot to identify if there is any correlation between any political party and sales of lait equitable. Our aim is to show that there might be a link with milk sales and a certain political party: are the sales correlated to a certain party presence?
```{r}
# Calculate correlation coefficients for each party
correlation_df <- data.frame(Party = c("PLR", "PS", "UDC", "Centre", "Verts", "Vertliberaux"),
                             Correlation = sapply(merged_data[, 4:9], function(x) cor(x, merged_data$`2023`)))

# Print the correlation coefficients
print(correlation_df)

# Create a matrix of plots for each party
party_plots <- lapply(names(merged_data)[4:9], function(party) {
  ggplot(merged_data, aes_string(x = "`2023`", y = party)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    labs(x = "Annual Sales", y = paste(party, "Party Presence (%)"), title = paste("Correlation:", party, "Party vs. Sales")) +
    theme_minimal()
})

# Arrange the plots in a matrix layout
matrix_plot <- gridExtra::grid.arrange(grobs = party_plots, ncol = 2)
matrix_plot
```

We can now notice that there is no specific correlation between a certain political party and sales of Lait équitable.

We will therefore proceed to select only the Locations where a political party has more than 20% presence, and then sum all the sales per political party, to see which party has the most influence over sales.

```{r}

# Read sales data from data.qmd
sales_data_2023 <- sales_data %>%
  select(-`2022`)

# Filter party data to keep only values above 20
filtered_party_data <- party_data %>%
  filter(PLR > 20 | PS > 20 | UDC > 20 | Centre > 20 | Verts > 20 | Vertliberaux > 20)

# Create separate datasets for each political party
plr_data <- filtered_party_data %>%
  filter(PLR > 20) %>%
  select(Location, PLR)

ps_data <- filtered_party_data %>%
  filter(PS > 20) %>%
  select(Location, PS)

udc_data <- filtered_party_data %>%
  filter(UDC > 20) %>%
  select(Location, UDC)

centre_data <- filtered_party_data %>%
  filter(Centre > 20) %>%
  select(Location, Centre)

verts_data <- filtered_party_data %>%
  filter(Verts > 20) %>%
  select(Location, Verts)

vertliberaux_data <- filtered_party_data %>%
  filter(Vertliberaux > 20) %>%
  select(Location, Vertliberaux)

# Merge each party's data with sales data for 2023
plr_sales <- merge(sales_data_2023, plr_data, by.x = "Location")
ps_sales <- merge(sales_data_2023, ps_data, by.x = "Location")
udc_sales <- merge(sales_data_2023, udc_data, by.x = "Location")
centre_sales <- merge(sales_data_2023, centre_data, by.x = "Location")
verts_sales <- merge(sales_data_2023, verts_data, by.x = "Location")
vertliberaux_sales <- merge(sales_data_2023, vertliberaux_data, by.x = "Location")

# Calculate total sales for each party
plr_total_sales <- sum(plr_sales$`2023`)
ps_total_sales <- sum(ps_sales$`2023`)
udc_total_sales <- sum(udc_sales$`2023`)
centre_total_sales <- sum(centre_sales$`2023`)
verts_total_sales <- sum(verts_sales$`2023`)

# Create a data frame for total sales by party
total_sales_df <- data.frame(Party = c("PLR", "PS", "UDC", "Centre", "Verts"),
                             Total_Sales = c(plr_total_sales, ps_total_sales, udc_total_sales,
                                             centre_total_sales, verts_total_sales))

# Define colors for each party
party_colors <- c("PLR" = "blue", "PS" = "red", "UDC" = "darkgreen", "Centre" = "orange", "Verts" = "green")

# Sort the data frame by Total_Sales in descending order
total_sales_df <- total_sales_df[order(-total_sales_df$Total_Sales), ]

# Plot total sales by party in descending order with specified colors
ggplot(total_sales_df, aes(x = reorder(Party, -Total_Sales), y = Total_Sales, fill = Party)) +
  geom_bar(stat = "identity") +
  labs(title = "Total Sales by Political Party in 2023", x = "Party", y = "Total Sales") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = party_colors)
```

We can see that there actually is a lot of sales of lait équitable where PS is present more than 20%

#### Correlating average revenue with Milk Sales

Now, we want to see if there is some correlation between the income per taxpayer of a commune and its sales.
```{r}
# Create a scatterplot
ggplot(merged_df, aes(x = `Revenu/contribuable`, y = `2022`)) +
  geom_point(aes(color = Location)) +
  labs(x = "Revenu/contribuable", y = "Sales 2022", title = "Relationship between Revenu/contribuable and Sales in 2022") +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()

# Create another scatterplot for 2023
ggplot(merged_df, aes(x = `Revenu/contribuable`, y = `2023`)) +
  geom_point(aes(color = Location)) +
  labs(x = "Revenu/contribuable", y = "Sales 2023", title = "Relationship between Revenu/contribuable and Sales in 2023") +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
```
Here we can again see that there is no specific correlation, and that the correlation is even negative. We will then move on to our last part where we will try to correlate the sales of lait équitable with the proximity of milk producers.

#### Correlating Distance to Producers 

Does being close to a Producer influence Sales ?
What we do here is to calculate a distance matrix in order to determine the proximity of each city to the producers. We then analyze the correlation between the total sales and the minimum distance to the producer. This analysis helps us understand if the proximity to the producer has any significant impact on the sales of Lait Equitable products.

The distance Matrix is shown as follow : 
```{python}
from geopy.distance import geodesic
latitudes = []
longitudes = []

# Parse each location and extract latitude and longitude
for location in locations:
    lat, lon = location.split(',')
    latitudes.append(float(lat))
    longitudes.append(float(lon))

# Create a DataFrame using pandas
producers = pd.DataFrame({
    'Latitude': latitudes,
    'Longitude': longitudes
})

#get the df from the python code
cities = df['City']

# Initialize an empty DataFrame to store distances
distance_matrix = pd.DataFrame(index=cities, columns=[f"Producer {i+1}" for i in range(len(producers))])

# Calculate distances and fill the DataFrame
for city in cities:
    city_lat, city_lon = get_lat_lon(city)
    if city_lat is not None and city_lon is not None:
        city_coords = (city_lat, city_lon)
        for index, producer in producers.iterrows():
            producer_coords = (producer['Latitude'], producer['Longitude'])
            distance = geodesic(city_coords, producer_coords).kilometers  # distance in kilometers
            distance_matrix.loc[city, f"Producer {index+1}"] = distance
            

# Flatten the DataFrame to get all distance values in one series
all_distances = distance_matrix.values.flatten()
```

```{r}
#get the distance_matrix in r
distance_matrix <- py$distance_matrix

#use reactable to show the table 
#show it using reactable
reactable(
  distance_matrix,  
  highlight = TRUE,  # Highlight rows on hover
  defaultPageSize = 10,  # Display 10 rows per page
  paginationType = "numbers",  # Use numbers for page navigation
  searchable = TRUE,  # Make the table searchable
  sortable = TRUE,  # Allow sorting
  resizable = TRUE  # Allow column resizing
)
```

We can see here the distribution of distances between cities and producers. The histogram shows the frequency of distances between cities and producers, providing insights into the geographical distribution of producers and their proximity to cities.


| Statistic         | Distance     |
|-------------------|--------------|
| Minimum Distance  | 0.68 km      |
| Maximum Distance  | 283.46 km    |
| Average Distance  | 116.73 km    |
| Median Distance   | 117.62 km    |

```{python}
# Basic statistics
print("Distance Statistics:")
print("Minimum Distance: {:.2f} km".format(all_distances.min()))
print("Maximum Distance: {:.2f} km".format(all_distances.max()))
print("Average Distance: {:.2f} km".format(all_distances.mean()))
print("Median Distance: {:.2f} km".format(np.median(all_distances)))

# Histogram of the distances
plt.figure(figsize=(10, 6))
plt.hist(all_distances, bins=30, color='#24918d', alpha=0.7)
plt.title('Distribution of Distances Between Cities and Producers')
plt.xlabel('Distance in km')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()
```

We see that the correlation is non-existent between the total sales and the distance to the producer. This suggests that the proximity to the producer does not significantly influence the sales of Lait Equitable products in the analyzed locations.

| Correlation between Total Sales 2022 and Min Distance to Producer: | Correlation between Total Sales 2023 and Min Distance to Producer: |
|------------------------------------|------------------------------------|
| 1.000000                                                           | 0.014807                                                           |
| 0.014807                                                           | 1.000000 
```{python}
df_sales = r.get('df_merged_sales')
#rename 'Location' column as 'City'
df_sales.rename(columns={'Location': 'City'}, inplace=True)
df_sales.set_index('City', inplace=True)

# Calculate the minimum distance for each city and add it to df_sales
df_sales['Min Distance to Producer'] = distance_matrix.min(axis=1)

# Calculate Pearson correlation
correlation_2022 = df_sales[['Total Sales 2022', 'Min Distance to Producer']].corr(method='pearson')
correlation_2023 = df_sales[['Total Sales 2023', 'Min Distance to Producer']].corr(method='pearson')

print("Correlation between Total Sales 2022 and Min Distance to Producer:")
print(correlation_2022)

print("Correlation between Total Sales 2023 and Min Distance to Producer:")
print(correlation_2023)

# Convert 'Total Sales 2022' and 'Min Distance to Producer' to numeric types explicitly
df_sales['Total Sales 2022'] = pd.to_numeric(df_sales['Total Sales 2022'], errors='coerce')
df_sales['Min Distance to Producer'] = pd.to_numeric(df_sales['Min Distance to Producer'], errors='coerce')


# Plotting Total Sales 2022 vs. Min Distance to Producer
plt.figure(figsize=(10, 6))
sns.regplot(
    x='Min Distance to Producer', 
    y='Total Sales 2022', 
    data=df_sales,
    scatter_kws={'s': 50, 'color': '#7e57c2'},  # Customizing the scatter plot points
    line_kws={'color': '#33848D', 'lw': 2}  # Customizing the regression line
)
plt.title('Total Sales 2022 vs. Minimum Distance to Producer')
plt.xlabel('Minimum Distance to Producer (km)')
plt.ylabel('Total Sales 2022')
plt.grid(True)
plt.show()

# Plot for 2023
plt.figure(figsize=(10, 6))
sns.regplot(x='Min Distance to Producer', y='Total Sales 2023', data=df_sales, 
            scatter_kws={'s': 50, 'color': '#33848D'}, line_kws={'color': '#7e57c2'})
plt.title('Total Sales 2023 vs. Min Distance to Producer')
plt.xlabel('Minimum Distance to Producer (km)')
plt.ylabel('Total Sales 2023')
plt.grid(True)
plt.show()
```


#### Diving Deeper

To dive a bit deeper into the insights on how the sales of each manor is influenced, we'll write a Python script using the `pandas` library. The goal is to iterate through each city (column in the distance_matrix DataFrame), find the producer (row) with the minimum distance for that city, and then tally the number of times each producer is the closest to any city. Finally, we'll create a DataFrame to display the number of times each producer was closest to a city.

Here is a step-by-step guide and the corresponding code:

Import the pandas library. Load the data into a DataFrame: We'll assume the data you provided is in a CSV or Excel file. If it's in another format, you can adjust the loading method accordingly. Initialize a DataFrame to keep track of the scores for each producer. Iterate through each column (city) in the distance_matrix_t DataFrame, find the index of the minimum distance, and update the score for the respective producer. Display the final DataFrame with the scores.

```{python}
# Initialize a DataFrame to store scores for each producer
scores = pd.DataFrame(0, index=distance_matrix.index, columns=['Score'])

# Iterate through each city (column) to find the producer with the minimum distance
for city in distance_matrix.columns:
    min_distance_producer = distance_matrix[city].idxmin()
    scores.loc[min_distance_producer, 'Score'] += 1

#merges scores with df_sales
df = df_sales.merge(scores, left_index=True, right_index=True, how='left')
correlation_matrix = df[['Total Sales 2022', 'Total Sales 2023', 'Score']].corr()

# Display the correlation matrix
correlation_matrix
```
Here is the correlation matrix for the total sales in 2022, total sales in 2023, and the score calculated based on the proximity of each producer to the cities

|                  | Total Sales 2022 | Total Sales 2023 | Score      |
|------------------|-------------------|-------------------|------------|
| Total Sales 2022 | 1.000000          | 0.994222          | 0.250883   |
| Total Sales 2023 | 0.994222          | 1.000000          | 0.268046   |
| Score            | 0.250883          | 0.268046          | 1.000000   |


It is better than the minimum distance correlation, but it is still not very high. This suggests that the proximity of a producer to a city, as measured by the score, has a moderate positive correlation with the total sales in both 2022 and 2023.


```{python}
# Plotting 'Sales 2022' vs 'Score'
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot
sns.regplot(x='Total Sales 2022', y='Score', data=df, ci=None, scatter_kws={'color': '#7e57c2'}, line_kws={'color': '#33848D'})
plt.title('Sales 2022 vs. Score')

# Plotting 'Sales 2023' vs 'Score'
plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot
sns.regplot(x='Total Sales 2023', y='Score', data=df, ci=None, scatter_kws={'color': '#33848D'}, line_kws={'color': '#7e57c2'})
plt.title('Sales 2023 vs. Score')

# Show the plots
plt.tight_layout()
plt.show()
```
